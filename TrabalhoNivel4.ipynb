{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "swmvZDRjpscH",
        "xgxsHq04qyqz",
        "zdkdYInrr2z5",
        "UzlO0Ynnr6_A"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPULf9Vc4kUaWnjAOnYHTtZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N1ckg4m3s/TRABALHOS_CURSO_FULLSTACK_5/blob/main/TrabalhoNivel4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICRO ATIVIDADE 2\n",
        "> Descrever tarefas diversas relacionadas ao\n",
        "Processamento de Linguagem Natural\n"
      ],
      "metadata": {
        "id": "swmvZDRjpscH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarefas diversas de Processamento de Linguagem Natural\n",
        "\n",
        "Passo 1: Instalando a biblioteca e recarregando o ambiente."
      ],
      "metadata": {
        "id": "M6cOHIA3pxsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "\n",
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "M9zrtrF7p6kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 2: Baixando o\n",
        "modelo a ser utilizado e recarregando o ambiente."
      ],
      "metadata": {
        "id": "XEpoQ47Op-nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"en_core_web_trf\")\n",
        "\n",
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "Gn30sVziqALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Importando a biblioteca e\n",
        "definindo o modelo a ser utilizado."
      ],
      "metadata": {
        "id": "UaSW4PORqEM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "y7PrFwE8qHNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 4: Definindo o texto a ser utilizado\n",
        "nas tarefas de NLP"
      ],
      "metadata": {
        "id": "ulOtLTSzqJbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"National Park Week starts on Saturday, and it also starts off with a bang-\n",
        "for-your-buck.\n",
        "\n",
        "That’s because every US National Park Service site will have free entry on\n",
        "Saturday. NPS manages almost 430 sites, and the majority of them already offer\n",
        "free entry every day.\n",
        "\n",
        "But this is your chance to get into the coveted, big-name national parks and other\n",
        "sites without paying a fee.\n",
        "\n",
        "That includes legendary parks such as Yosemite, which normally has an entry fee\n",
        "of $20 per person or $35 per vehicle.\n",
        "\n",
        "(Note: You’ll still need a reservation to drive into Yosemite on weekends and\n",
        "holidays from April 13 to June 30.)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "wAi8D76gqKa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 5: Realizando a tokenização do\n",
        "texto"
      ],
      "metadata": {
        "id": "55qUbRz2qPxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "rdP8TXzAqRsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 6: Obtendo as tags de classe\n",
        "gramatical para os tokens individuais."
      ],
      "metadata": {
        "id": "NILKXCt1qVKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "\n",
        "    # Print the token and the POS tags\n",
        "\n",
        "    print(token, token.pos_, token.tag_)\n",
        "\n"
      ],
      "metadata": {
        "id": "Kig7Ev1UqX53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 7: Imprimindo os tokens e sua análise morfológica"
      ],
      "metadata": {
        "id": "7OeNJY2wqaw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "\n",
        "    print(token, token.morph)\n",
        "\n"
      ],
      "metadata": {
        "id": "EX0a8UAmqcJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 8: Visualizando a árvore de análise sintática por frase\n",
        "e a relação entre as palavras."
      ],
      "metadata": {
        "id": "76HIzpnBqeB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style='dep', options={'compact': True})"
      ],
      "metadata": {
        "id": "tKj-nGcgqfTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICRO ATIVIDADE 3\n",
        "> Descrever o processo de identificação de\n",
        "entidades a partir de textos"
      ],
      "metadata": {
        "id": "xgxsHq04qyqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando as bibliotecas python e configurando o modelo a ser utilizado pelo\n",
        "spacy."
      ],
      "metadata": {
        "id": "oIBXx71-q7DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 200)"
      ],
      "metadata": {
        "id": "97qLLQhIq7VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição do texto a\n",
        "ser analisado e exibição das entidades encontradas"
      ],
      "metadata": {
        "id": "PUYmJQGhq9iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "gUHmwSEnq_5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando a lib displacy e\n",
        "exibindo o resultado da análise NER de forma visual"
      ],
      "metadata": {
        "id": "pn9Q0ylrrGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "LQJB_dJhrJhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisando um bloco de texto"
      ],
      "metadata": {
        "id": "dQKobwPWrLIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_txt = \"\"\"\n",
        "\n",
        "Hello Zhang Wei. Your AnyCompany Financial Services, LLC credit card account\n",
        "\n",
        "1111-0000-1111-0000 has a minimum payment of $24.53 that is due by July 31st.\n",
        "\n",
        "Based on your autopay settings, we will withdraw your payment on the due date\n",
        "from\n",
        "\n",
        "your bank account XXXXXX1111 with the routing number XXXXX0000.\n",
        "\n",
        "\n",
        "\n",
        "Your latest statement was mailed to 100 Main Street, Anytown, WA 98121.\n",
        "\n",
        "After your payment is received, you will receive a confirmation text message\n",
        "\n",
        "at 206-555-0100.\n",
        "\n",
        "If you have questions about your bill, AnyCompany Customer Service is available\n",
        "by\n",
        "\n",
        "phone at 206-555-0199 or email at support@anycompany.com.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "newdoc = nlp(sample_txt)\n",
        "\n",
        "displacy.render(newdoc, style=\"ent\")\n",
        "\n"
      ],
      "metadata": {
        "id": "g92Mv0X6rNMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizando o resultado da\n",
        "análise de forma tabular"
      ],
      "metadata": {
        "id": "VJyCjYlQrUjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = [(ent.text, ent.label_, ent.lemma_) for ent in newdoc.ents]\n",
        "\n",
        "df = pd.DataFrame(entities, columns=['text', 'type', 'lemma'])\n",
        "\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "-uYN5d72rWc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando modelos no idioma\n",
        "português"
      ],
      "metadata": {
        "id": "AEEDkbMKraH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt\n",
        "\n",
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "SIdPUOnurcEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "txt_br=\"\"\"\n",
        "\n",
        "ONU aprova missão internacional para restabelecer segurança no Haiti\n",
        "\n",
        "O Conselho de Segurança das Nações Unidas (ONU) aprovou na noite de segunda-\n",
        "feira (2) a criação e o envio de uma força internacional para a manutenção de paz\n",
        "no Haiti, devido aos conflitos entre as gangues que dominam o país.\n",
        "\n",
        "O Haiti é o país mais pobre do hemisfério ocidental. A história do país é marcada\n",
        "por golpes, deposições e massacres que geraram grande instabilidade política,\n",
        "turbulência econômica e crise social. Em 30 de abril de 2004 o Conselho de\n",
        "Segurança da Organização das Nações Unidas aprovou, por unanimidade, a\n",
        "criação da Missão de Estabilização do Haiti, a MINUSTAH.\n",
        "\n",
        "A missão, planejada para ter uma duração inicial de seis meses, vem sendo\n",
        "prorrogada. O objetivo é combater a insegurança no país após a crise que forçou a\n",
        "saída do ex-presidente Jean Bertrand Aristide, em fevereiro de 2004.\n",
        "\n",
        "Coube ao Brasil a chefia da missão, instalada no mês de junho de 2004, bem como\n",
        "constituir o maior contingente nacional de \"boinas azuis\" : 1.470 militares, de um\n",
        "total de 6.700, militares de 21 países. Apesar das dificuldades estruturais do Haiti,\n",
        "as atividades da MINUSTAH trouxeram avanços, como a realização das eleições\n",
        "ocorridas em fevereiro deste ano.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "docbr = nlp(txt_br)\n",
        "displacy.render(docbr, style=\"ent\")"
      ],
      "metadata": {
        "id": "r9Jlq0t8rd34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICRO ATIVIDADE 4\n",
        "> Descrever o processo de extração de frases-\n",
        "chave a partir de textos"
      ],
      "metadata": {
        "id": "zdkdYInrr2z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extração de frases-chave a partir de textos.\n",
        "\n",
        "Passo 1: Instalação das bibliotecas.."
      ],
      "metadata": {
        "id": "S6FPT9lBsiv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "cjwGDGBKsjzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 2: Importando a lib pke e inicializando\n",
        "o modelo de extração"
      ],
      "metadata": {
        "id": "DMbndsWGsmfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# initialize a TopicRank keyphrase extraction model\n",
        "\n",
        "extractor = pke.unsupervised.TopicRank()"
      ],
      "metadata": {
        "id": "NKstbfOVsoP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Definindo e carregando o texto\n",
        "a ser analisado."
      ],
      "metadata": {
        "id": "QYSaA1zMsqW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"Tesla has been ordered to recall nearly 4,000 of its Cybertrucks due to\n",
        "an accelerator pedal that\n",
        "\n",
        "can stick in place when pressed down.\n",
        "\n",
        "The cause, according to the regulator: soap.\n",
        "\n",
        "“An unapproved change introduced lubricant (soap) to aid in the component\n",
        "assembly of the pad onto the accelerator pedal.\n",
        "\n",
        "Residual lubricant reduced the retention of the pad to the pedal,” the NHTSA\n",
        "wrote in the recall document.\n",
        "\n",
        "Tesla has yet to detail how many of the futuristic looking Cybertrucks it has\n",
        "produced. But it has said that it would\n",
        "\n",
        "be slow ramping up production of the vehicle, which had its first deliveries in late\n",
        "November.\n",
        "\n",
        "The NHTSA said the recall affects “all Model Year (‘MY’) 2024 Cybertruck vehicles\n",
        "manufactured from November 13, 2023, to April 4, 2024.”\n",
        "\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "extractor.load_document(input=sample, language='en')\n"
      ],
      "metadata": {
        "id": "AtsSRS7Nsry-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 4: Imprimindo as informações das\n",
        "sentenças\n"
      ],
      "metadata": {
        "id": "ud2Ll3zeszE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(extractor.sentences):\n",
        "# print out the sentence id, its tokens, its stems and the corresponding Part-of-Speech tags\n",
        "\n",
        "    print(\"sentence {}:\".format(i))\n",
        "\n",
        "    print(\" - words: {} ...\".format(' '.join(sentence.words[:5])))\n",
        "\n",
        "    print(\" - stems: {} ...\".format(' '.join(sentence.stems[:5])))\n",
        "\n",
        "    print(\" - PoS: {} ...\".format(' '.join(sentence.pos[:5])))"
      ],
      "metadata": {
        "id": "2bk0p9Cys4S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 5: Identificando as frases-\n",
        "chave candidatas"
      ],
      "metadata": {
        "id": "LTLeAVK2s40d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.candidate_selection()\n",
        "\n",
        "for i, candidate in enumerate(extractor.candidates):\n",
        "\n",
        "\n",
        "\n",
        "    # print out the candidate id, its stemmed form\n",
        "\n",
        "    print(\"candidate {}: {} (stemmed form)\".format(i, candidate))\n",
        "\n",
        "\n",
        "\n",
        "    # print out the surface forms of the candidate\n",
        "\n",
        "    print(\" - surface forms:\", [ \" \".join(u) for u in\n",
        "extractor.candidates[candidate].surface_forms])\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding offsets\n",
        "\n",
        "    print(\" - offsets:\", extractor.candidates[candidate].offsets)\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding sentence ids\n",
        "\n",
        "    print(\" - sentence_ids:\", extractor.candidates[candidate].sentence_ids)\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding PoS patterns\n",
        "\n",
        "    print(\" - pos_patterns:\", extractor.candidates[candidate].pos_patterns)\n",
        "\n"
      ],
      "metadata": {
        "id": "X9jsb8iTs6jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 6: Ranqueando as palavras-\n",
        "chave candidatas"
      ],
      "metadata": {
        "id": "2fZ3uvYhs8P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.candidate_weighting()\n",
        "\n",
        "for i, topic in enumerate(extractor.topics):\n",
        "    # print out the topic id and the candidates it groups together\n",
        "\n",
        "    print(\"topic {}: {} \".format(i, ';'.join(topic)))"
      ],
      "metadata": {
        "id": "YgZi_BtQs9d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICRO ATIVIDADE 5\n",
        "> Descrever o processo de identificação de\n",
        "linguagem predominante a partir de textos\n",
        "\n"
      ],
      "metadata": {
        "id": "UzlO0Ynnr6_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identificação de idioma predominante em textos.\n",
        "\n",
        "Passo 1: Instalação das bibliotecas"
      ],
      "metadata": {
        "id": "TyblOYh3tXEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "vsQ5jRZ8tXZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 2: Definindo as frases, em diferentes\n",
        "idiomas, que serão analisadas."
      ],
      "metadata": {
        "id": "zbowWduStXtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Wear masks, keep distance, wash hands, be safe in these difficult days.\" ,\n",
        "\n",
        "        \"Viseljen maszkot, tartson távolságot, mosson kezet, legyen biztonságban ezekben a nehéz napokban\",\n",
        "\n",
        "        \"Deaths are increasing, be vigilent.\",\n",
        "\n",
        "        \"Носите маски, соблюдайте дистанцию, мойте руки, будьте осторожны в эти тяжелые дни.\",\n",
        "\n",
        "        \"Covid-19: Indians flock to vaccination centers as vaccines are now available for 60+ in India since the 1st of March\",\n",
        "\n",
        "        \"Indossa maschere, mantieni le distanze, lavati le mani, sii al sicuro in questi giorni difficili.\",\n",
        "\n",
        "        \"Portez des masques, gardez vos distances, lavez-vous les mains, soyez en sécurité en ces jours difficiles.\",\n",
        "\n",
        "        \"Brug masker, hold afstand, vask hænder, vær sikker i disse vanskelige dage.\",\n",
        "\n",
        "        \"We are facing a global education crisis. No effort should be spared to safely bring every child back into the classroom.\",\n",
        "\n",
        "        \"Bruk masker, hold avstand, vask hendene, vær trygg i disse vanskelige dagene.\",\n",
        "\n",
        "        \"Portu maskojn, tenu distancon, lavu manojn, estu sekuraj en ĉi tiuj malfacilaj tagoj.\",\n",
        "\n",
        "        \"Tragen Sie Masken, halten Sie Abstand, waschen Sie Ihre Hände, seien Sie in diesen schwierigen Tagen sicher.\",\n",
        "\n",
        "        \"Носіть маски, тримайтеся на відстані, мийте руки, будьте в безпеці в ці важкі дні.\",\n",
        "\n",
        "        \"Lock down , working from home are the keys words for these days.\",\n",
        "\n",
        "        \"Lives have changed drastically across the planet and this period will foreverbe remembered as the beginning of something we have yet to witness.\",\n",
        "\n",
        "        \"Este é um exemplo de texto escrito em português.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "Cu_a2-FHtX-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Recarragendo o ambiente\n",
        "python após a instalação da lib."
      ],
      "metadata": {
        "id": "reuLaxMHtYTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "npk6-N37tYlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 4: Detectando a linguagem\n",
        "predominante na lista de frases."
      ],
      "metadata": {
        "id": "OOZ11_gptzfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "\n",
        "for x in text:\n",
        "\n",
        "    print ('Frase: ', x)\n",
        "\n",
        "    print ('Idioma: ', detect(x), '\\n\\n')"
      ],
      "metadata": {
        "id": "k74vppNyt1Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rcp51dOot23U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PG-vOn_7Z9sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TRABALHO**"
      ],
      "metadata": {
        "id": "a3IOsH45uCsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo para Análise de Sentimentos\n",
        "\n",
        "Passo 1: Nesse passo iremos Instalar as bibliotecas necessarias e recarregar o ambiente"
      ],
      "metadata": {
        "id": "57pCFxZFZ3xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install spacytextblob"
      ],
      "metadata": {
        "id": "GU3p8MKAaD7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "id": "bbDE6mtsuITD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b64dc2-c6b7-4ea4-b195-69b8ce269e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pkg_resources' from '/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 2: Importando as\n",
        "bibliotecas para que possa ser possivel análisar dos sentimento dos Tweets"
      ],
      "metadata": {
        "id": "tqUbvgtzaHkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob"
      ],
      "metadata": {
        "id": "-iDlzIBaaK73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Definindo o modelo e a\n",
        "pipeline a serem utilizadas na análise"
      ],
      "metadata": {
        "id": "0o95NZo2aOF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nlp.add_pipe('spacytextblob')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJTm37tUaPjP",
        "outputId": "563dcc13-2cb4-4183-e5df-9bfa6f3fc958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7b90e3e87d60>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 4: Definindo o texto inicial a\n",
        "ser analisado para verificação/validação da biblioteca se esta tudo em ordem."
      ],
      "metadata": {
        "id": "5tCeUSyYaT6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = 'This is a wonderful campsite. I loved the serenity and the birds chirping in the morning.'\n",
        "\n",
        "doc = nlp(user_input)"
      ],
      "metadata": {
        "id": "KZgUBvXYaUS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 5: Exibindo o resultado entre -1 e 1 da\n",
        "primeira análise, sendo:\n",
        "* -1 = Tweet [Triste/ruim]\n",
        "*  0 = Tweet [Neutro]\n",
        "*  1 = Tweet [Feliz/Bom]"
      ],
      "metadata": {
        "id": "BzQ7aQy_abUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_polarity = doc._.polarity\n",
        "sentiment = {\n",
        "    'score': input_polarity\n",
        "}\n",
        "print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgc2F9ppagle",
        "outputId": "e0bd02a2-095c-4bc2-8377-b91c86d7e1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.85}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 6: Listando alguns Tweets para analizar o sentimento de cada um"
      ],
      "metadata": {
        "id": "GuMqPC5AalCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=[\n",
        "    '''Bayer Leverkusen goalkeeper Bernd Leno will not be going to Napoli. His\n",
        "    agent Uli Ferber to Bild: I can confirm that there were negotiations with Napoli,\n",
        "    which we have broken off. Napoli is not an option. Atletico Madrid and Arsenal are\n",
        "    the other strong rumours. #B04 #AFC''',\n",
        "\n",
        "\n",
        "    '''Gary Speed v Blackburn at St James in 2001/02 anyone? #NUFC #BEL #JAP\n",
        "    #WorldCup''',\n",
        "\n",
        "    '''@ChelseaFC Don't make him regret it and start him over Hoofiz''',\n",
        "\n",
        "    '''@LiverpoolFF @AnfieldEdition He's a liar, made up. I've unfollowed him as loads\n",
        "    of others have. Pure blagger. #LFC''',\n",
        "\n",
        "    '''@theesk @Everton Didn't realise Kenwright is due to leave at the end of the\n",
        "    month. In all seriousness could you see him being interested in us?''',\n",
        "\n",
        "    '''@hasanshahbaz19 @LFC My knowledge has decreased somewhat in the past few\n",
        "    seasons''',\n",
        "\n",
        "    '''Report: Linked with #Everton and #Wolves, Italians set to sign £4.5m-rated\n",
        "    winger''',\n",
        "\n",
        "    '''Am seeing tweets that there’s been a fall out @Everton between the money men...\n",
        "    I’m presuming it’s just a quiet news day or some kopite with nothing better to do!\n",
        "    @ALANMYERSMEDIA''',\n",
        "\n",
        "    '''@LFC @officialAL20 @IntChampionsCup @ManUtd Expect loads of excuses after\n",
        "    tonight’s game''',\n",
        "\n",
        "    '''@MartinDiamond17 @azryahmad @Baren_D @Mathewlewis1997 @iamheinthu\n",
        "    @DiMarzio @Alissonbecker @LFC @SkySportsNews @SkySport @OfficialASRoma\n",
        "    I’m just fine I have your fanbase angry over stating facts should ask them hun. Xo''',\n",
        "\n",
        "    '''What a weekend of football results! @ManUtd @Glentoran @RangersFC &amp;\n",
        "    Hearts ????''',\n",
        "\n",
        "    '''@ChelseaFC For the first time in a long while, my heart was relaxed while\n",
        "    watching Chelsea. Really enjoyed it today. Come on, CHELSEA!!!''',\n",
        "\n",
        "    '''@ChelseaFC @CesarAzpi What a fantastic signing worth every single penny ??''',\n",
        "\n",
        "    '''Pogba scores, Pogba assists. But tomorrow papers won't be telling you this,\n",
        "    instead they will tell you how he'll end up at Juve because he's unhappy,\n",
        "    frustrated, have grudges with Mourinho and so on and so forth #mufc''',\n",
        "\n",
        "    '''@WestHamUtd we need to keep @CH14_ and get @HirvingLozano70 to\n",
        "    compliment''',\n",
        "\n",
        "    '''@kevdev9 @Everton Shouldn’t be happening! Needs to stay away with his\n",
        "    venomous attitude until he is sold!''',\n",
        "\n",
        "    '''@brfootball @aguerosergiokun @ManCity What a genius. Pep taking winning\n",
        "    mentality with him, conquering league after league. Baller''',\n",
        "\n",
        "    '''@HMZ0709  Can we get a RT for our #lfc Mo Salah Liverpool Enamel Pin Badge''',\n",
        "\n",
        "    '''PERFECT DAY''',\n",
        "\n",
        "    '''i'm loving this day''',\n",
        "\n",
        "    '''I HATE TODAY''']\n",
        "\n"
      ],
      "metadata": {
        "id": "z3WN4PZIalR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 7: Analisando os tweets"
      ],
      "metadata": {
        "id": "wBHtvtvKaz_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tweets:\n",
        "  doc = nlp(item)\n",
        "  input_polarity = doc._.polarity\n",
        "  sentiment = {\n",
        "      'tweet': item,\n",
        "      'score': input_polarity\n",
        "  }\n",
        "  print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OYOlohKa0QO",
        "outputId": "aaeb3915-a409-4e6c-a8df-8d7e061aa30c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tweet': 'Bayer Leverkusen goalkeeper Bernd Leno will not be going to Napoli. His\\n    agent Uli Ferber to Bild: I can confirm that there were negotiations with Napoli,\\n    which we have broken off. Napoli is not an option. Atletico Madrid and Arsenal are\\n    the other strong rumours. #B04 #AFC', 'score': -0.030555555555555575}\n",
            "{'tweet': 'Gary Speed v Blackburn at St James in 2001/02 anyone? #NUFC #BEL #JAP\\n    #WorldCup', 'score': 0.0}\n",
            "{'tweet': \"@ChelseaFC Don't make him regret it and start him over Hoofiz\", 'score': 0.0}\n",
            "{'tweet': \"@LiverpoolFF @AnfieldEdition He's a liar, made up. I've unfollowed him as loads\\n    of others have. Pure blagger. #LFC\", 'score': 0.21428571428571427}\n",
            "{'tweet': \"@theesk @Everton Didn't realise Kenwright is due to leave at the end of the\\n    month. In all seriousness could you see him being interested in us?\", 'score': 0.0625}\n",
            "{'tweet': '@hasanshahbaz19 @LFC My knowledge has decreased somewhat in the past few\\n    seasons', 'score': -0.2833333333333334}\n",
            "{'tweet': 'Report: Linked with #Everton and #Wolves, Italians set to sign £4.5m-rated\\n    winger', 'score': 0.0}\n",
            "{'tweet': 'Am seeing tweets that there’s been a fall out @Everton between the money men...\\n    I’m presuming it’s just a quiet news day or some kopite with nothing better to do!\\n    @ALANMYERSMEDIA', 'score': 0.3125}\n",
            "{'tweet': '@LFC @officialAL20 @IntChampionsCup @ManUtd Expect loads of excuses after\\n    tonight’s game', 'score': -0.4}\n",
            "{'tweet': '@MartinDiamond17 @azryahmad @Baren_D @Mathewlewis1997 @iamheinthu\\n    @DiMarzio @Alissonbecker @LFC @SkySportsNews @SkySport @OfficialASRoma\\n    I’m just fine I have your fanbase angry over stating facts should ask them hun. Xo', 'score': -0.04166666666666666}\n",
            "{'tweet': 'What a weekend of football results! @ManUtd @Glentoran @RangersFC &amp;\\n    Hearts ????', 'score': 0.0}\n",
            "{'tweet': '@ChelseaFC For the first time in a long while, my heart was relaxed while\\n    watching Chelsea. Really enjoyed it today. Come on, CHELSEA!!!', 'score': 0.39218749999999997}\n",
            "{'tweet': '@ChelseaFC @CesarAzpi What a fantastic signing worth every single penny ??', 'score': 0.20952380952380953}\n",
            "{'tweet': \"Pogba scores, Pogba assists. But tomorrow papers won't be telling you this,\\n    instead they will tell you how he'll end up at Juve because he's unhappy,\\n    frustrated, have grudges with Mourinho and so on and so forth #mufc\", 'score': -0.6499999999999999}\n",
            "{'tweet': '@WestHamUtd we need to keep @CH14_ and get @HirvingLozano70 to\\n    compliment', 'score': 0.0}\n",
            "{'tweet': '@kevdev9 @Everton Shouldn’t be happening! Needs to stay away with his\\n    venomous attitude until he is sold!', 'score': 0.0}\n",
            "{'tweet': '@brfootball @aguerosergiokun @ManCity What a genius. Pep taking winning\\n    mentality with him, conquering league after league. Baller', 'score': 0.5}\n",
            "{'tweet': '@HMZ0709  Can we get a RT for our #lfc Mo Salah Liverpool Enamel Pin Badge', 'score': 0.0}\n",
            "{'tweet': 'PERFECT DAY', 'score': 1.0}\n",
            "{'tweet': \"i'm loving this day\", 'score': 0.6}\n",
            "{'tweet': 'I HATE TODAY', 'score': -0.8}\n"
          ]
        }
      ]
    }
  ]
}